{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Intro","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:20:20.326015Z","iopub.execute_input":"2025-05-04T14:20:20.326841Z","iopub.status.idle":"2025-05-04T14:20:20.334636Z","shell.execute_reply.started":"2025-05-04T14:20:20.326804Z","shell.execute_reply":"2025-05-04T14:20:20.333442Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"#import libraries to be used \n\n#general\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\n\n#visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nimport sklearn.linear_model\nimport sklearn.metrics\nimport sklearn.neighbors\nimport sklearn.preprocessing\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n#from textblob import TextBlob\n\n#machine learning model requirements\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve \nfrom sklearn import metrics\nfrom sklearn.metrics import (\n    f1_score, roc_auc_score, accuracy_score,\n    precision_recall_curve, roc_curve, average_precision_score\n)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.sparse import hstack\n\n\n#models\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:20:20.337109Z","iopub.execute_input":"2025-05-04T14:20:20.337486Z","iopub.status.idle":"2025-05-04T14:20:32.356471Z","shell.execute_reply.started":"2025-05-04T14:20:20.337442Z","shell.execute_reply":"2025-05-04T14:20:32.355379Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# List all files in the BABE dataset\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename.endswith('.xlsx'):\n            excel_path = os.path.join(dirname, filename)\n            print(\"Excel file found:\", excel_path)\n\n            # List all sheet names in that file\n            xls = pd.ExcelFile(excel_path)\n            print(\"Sheet names:\", xls.sheet_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:20:32.357656Z","iopub.execute_input":"2025-05-04T14:20:32.358437Z","iopub.status.idle":"2025-05-04T14:20:35.041691Z","shell.execute_reply.started":"2025-05-04T14:20:32.358400Z","shell.execute_reply":"2025-05-04T14:20:35.040793Z"}},"outputs":[{"name":"stdout","text":"Excel file found: /kaggle/input/babe-media-bias-annotations-by-experts/data/final_labels_SG1.xlsx\nSheet names: ['Sheet1']\nExcel file found: /kaggle/input/babe-media-bias-annotations-by-experts/data/final_labels_SG2.xlsx\nSheet names: ['Sheet1']\nExcel file found: /kaggle/input/babe-media-bias-annotations-by-experts/data/dt_final_SG1.xlsx\nSheet names: ['Sheet1']\nExcel file found: /kaggle/input/babe-media-bias-annotations-by-experts/data/final_labels_MBIC.xlsx\nSheet names: ['Sheet1']\nExcel file found: /kaggle/input/babe-media-bias-annotations-by-experts/data/raw_labels_SG1.xlsx\nSheet names: ['Sheet1']\nExcel file found: /kaggle/input/babe-media-bias-annotations-by-experts/data/dt_final_SG2.xlsx\nSheet names: ['Sheet1']\nExcel file found: /kaggle/input/babe-media-bias-annotations-by-experts/data/raw_labels_MBIC.xlsx\nSheet names: ['annotations']\nExcel file found: /kaggle/input/babe-media-bias-annotations-by-experts/data/raw_labels_SG2.xlsx\nSheet names: ['Sheet1']\nExcel file found: /kaggle/input/babe-media-bias-annotations-by-experts/data/bias_word_lexicon.xlsx\nSheet names: ['top100_10times']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Load just one worksheet by name\ndf = pd.read_excel('/kaggle/input/babe-media-bias-annotations-by-experts/data/dt_final_SG2.xlsx', sheet_name='Sheet1')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:20:35.042684Z","iopub.execute_input":"2025-05-04T14:20:35.043876Z","iopub.status.idle":"2025-05-04T14:24:47.370305Z","shell.execute_reply.started":"2025-05-04T14:20:35.043849Z","shell.execute_reply":"2025-05-04T14:24:47.369372Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:24:47.372562Z","iopub.execute_input":"2025-05-04T14:24:47.372828Z","iopub.status.idle":"2025-05-04T14:24:47.380892Z","shell.execute_reply.started":"2025-05-04T14:24:47.372809Z","shell.execute_reply":"2025-05-04T14:24:47.379765Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Index(['Unnamed: 0', 'sentence', 'outlet', 'topic', 'type', 'article',\n       'biased_words2', 'text', 'text_low', 'pos',\n       ...\n       'ne_NORP_context', 'ne_ORDINAL_context', 'ne_ORG_context',\n       'ne_PERCENT_context', 'ne_PERSON_context', 'ne_PRODUCT_context',\n       'ne_QUANTITY_context', 'ne_TIME_context', 'ne_WORK_OF_ART_context',\n       'ne_LANGUAGE_context'],\n      dtype='object', length=301)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"def evaluate_file(file_path, duplicate_column=None):\n    \"\"\"\n    Evaluates a given CSV or Excel file by:\n    - Printing general file information\n    - Checking for duplicate values in a specified column\n    - Searching for zero values\n    - Searching for empty (NaN) cells\n    \"\"\"\n\n    # Print file information\n    print(\"\\n=== File Info ===\")\n    print(file_path.info())\n\n    # Check for duplicates in the specified column\n    if duplicate_column:\n        duplicate_count = file_path.duplicated(subset=[duplicate_column]).sum()\n        print(f\"\\n=== Duplicates in '{duplicate_column}' ===\")\n        print(f\"Total duplicate values: {duplicate_count}\")\n    \n    # Check for zero values\n    zero_values = (file_path == 0).sum().sum()\n    print(f\"\\n=== Zero Values ===\")\n    print(f\"Total zero values: {zero_values}\")\n\n    # Check for empty (NaN) cells\n    missing_values = file_path.isnull().sum().sum()\n    print(f\"\\n=== Empty (NaN) Cells ===\")\n    print(f\"Total empty cells: {missing_values}\")\n    \n    #print a sample \n    display(file_path.sample(n=5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:24:47.381969Z","iopub.execute_input":"2025-05-04T14:24:47.382381Z","iopub.status.idle":"2025-05-04T14:24:47.405447Z","shell.execute_reply.started":"2025-05-04T14:24:47.382347Z","shell.execute_reply":"2025-05-04T14:24:47.404556Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#Run pipeline for each of the files to evaluate issues \nevaluate_file(df, duplicate_column=\"text\")  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:24:47.406310Z","iopub.execute_input":"2025-05-04T14:24:47.406588Z","iopub.status.idle":"2025-05-04T14:24:47.798299Z","shell.execute_reply.started":"2025-05-04T14:24:47.406556Z","shell.execute_reply":"2025-05-04T14:24:47.797358Z"}},"outputs":[{"name":"stdout","text":"\n=== File Info ===\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 65822 entries, 0 to 65821\nColumns: 301 entries, Unnamed: 0 to ne_LANGUAGE_context\ndtypes: bool(2), float64(4), int64(281), object(14)\nmemory usage: 150.3+ MB\nNone\n\n=== Duplicates in 'text' ===\nTotal duplicate values: 52531\n\n=== Zero Values ===\nTotal zero values: 17437766\n\n=== Empty (NaN) Cells ===\nTotal empty cells: 140258\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"       Unnamed: 0                                           sentence  \\\n5011         8322  An analysis by Fox News shows at least eight p...   \n53726       89616  The United States should learn from China and ...   \n29151       48769  Leonhardt’s column in the New York Times provi...   \n64523      108111  White supremacist violent extremists can gener...   \n150           238  [The police] now prefer to think of themselves...   \n\n          outlet              topic    type  \\\n5011    Fox News       middle-class   right   \n53726    Reuters        gun control  center   \n29151  Breitbart       middle-class   right   \n64523      MSNBC  white-nationalism    left   \n150    Breitbart  marriage-equality     NaN   \n\n                                                 article  \\\n5011   An analysis by Fox News shows at least eight p...   \n53726  The United States should learn from China and ...   \n29151  Leonhardt’s column in the New York Times provi...   \n64523  White supremacist violent extremists can gener...   \n150    [The police] now prefer to think of themselves...   \n\n                                          biased_words2    text text_low  \\\n5011                            ['drastic', 'overhaul']  system   system   \n53726                                                []   China    china   \n29151              ['enamored', 'chaotic', 'fractured']  column   column   \n64523  ['violent', 'virulent', 'supremacist', 'hatred']    anti     anti   \n150                                          ['wounds']  prance   prance   \n\n         pos  ... ne_NORP_context ne_ORDINAL_context ne_ORG_context  \\\n5011    NOUN  ...               0                  0              0   \n53726  PROPN  ...               0                  0              0   \n29151   NOUN  ...               0                  0              1   \n64523   NOUN  ...               1                  0              0   \n150     VERB  ...               0                  0              0   \n\n      ne_PERCENT_context  ne_PERSON_context  ne_PRODUCT_context  \\\n5011                   0                  0                   0   \n53726                  0                  0                   0   \n29151                  0                  0                   0   \n64523                  0                  0                   0   \n150                    0                  0                   0   \n\n       ne_QUANTITY_context  ne_TIME_context  ne_WORK_OF_ART_context  \\\n5011                     0                0                       0   \n53726                    0                0                       0   \n29151                    0                0                       0   \n64523                    0                0                       0   \n150                      0                0                       0   \n\n       ne_LANGUAGE_context  \n5011                     0  \n53726                    0  \n29151                    0  \n64523                    0  \n150                      0  \n\n[5 rows x 301 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>sentence</th>\n      <th>outlet</th>\n      <th>topic</th>\n      <th>type</th>\n      <th>article</th>\n      <th>biased_words2</th>\n      <th>text</th>\n      <th>text_low</th>\n      <th>pos</th>\n      <th>...</th>\n      <th>ne_NORP_context</th>\n      <th>ne_ORDINAL_context</th>\n      <th>ne_ORG_context</th>\n      <th>ne_PERCENT_context</th>\n      <th>ne_PERSON_context</th>\n      <th>ne_PRODUCT_context</th>\n      <th>ne_QUANTITY_context</th>\n      <th>ne_TIME_context</th>\n      <th>ne_WORK_OF_ART_context</th>\n      <th>ne_LANGUAGE_context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5011</th>\n      <td>8322</td>\n      <td>An analysis by Fox News shows at least eight p...</td>\n      <td>Fox News</td>\n      <td>middle-class</td>\n      <td>right</td>\n      <td>An analysis by Fox News shows at least eight p...</td>\n      <td>['drastic', 'overhaul']</td>\n      <td>system</td>\n      <td>system</td>\n      <td>NOUN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>53726</th>\n      <td>89616</td>\n      <td>The United States should learn from China and ...</td>\n      <td>Reuters</td>\n      <td>gun control</td>\n      <td>center</td>\n      <td>The United States should learn from China and ...</td>\n      <td>[]</td>\n      <td>China</td>\n      <td>china</td>\n      <td>PROPN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>29151</th>\n      <td>48769</td>\n      <td>Leonhardt’s column in the New York Times provi...</td>\n      <td>Breitbart</td>\n      <td>middle-class</td>\n      <td>right</td>\n      <td>Leonhardt’s column in the New York Times provi...</td>\n      <td>['enamored', 'chaotic', 'fractured']</td>\n      <td>column</td>\n      <td>column</td>\n      <td>NOUN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>64523</th>\n      <td>108111</td>\n      <td>White supremacist violent extremists can gener...</td>\n      <td>MSNBC</td>\n      <td>white-nationalism</td>\n      <td>left</td>\n      <td>White supremacist violent extremists can gener...</td>\n      <td>['violent', 'virulent', 'supremacist', 'hatred']</td>\n      <td>anti</td>\n      <td>anti</td>\n      <td>NOUN</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>150</th>\n      <td>238</td>\n      <td>[The police] now prefer to think of themselves...</td>\n      <td>Breitbart</td>\n      <td>marriage-equality</td>\n      <td>NaN</td>\n      <td>[The police] now prefer to think of themselves...</td>\n      <td>['wounds']</td>\n      <td>prance</td>\n      <td>prance</td>\n      <td>VERB</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 301 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Show columns with at least one NaN value\nnan_columns = df.columns[df.isna().any()].tolist()\n\nprint(\"Columns with NaN values:\", nan_columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:24:47.798953Z","iopub.execute_input":"2025-05-04T14:24:47.799200Z","iopub.status.idle":"2025-05-04T14:24:47.864787Z","shell.execute_reply.started":"2025-05-04T14:24:47.799179Z","shell.execute_reply":"2025-05-04T14:24:47.863975Z"}},"outputs":[{"name":"stdout","text":"Columns with NaN values: ['type', 'tfidf_art', 'ne_label', 'MRCP_concretness_ratings', 'MRCP_Imagability_ratings']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\n# identify which columns have one or more zeroes\nzero_columns = df.columns[(df == 0).any()].tolist()\n\nprint(\"Columns containing zeroes:\", zero_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:24:47.865651Z","iopub.execute_input":"2025-05-04T14:24:47.865868Z","iopub.status.idle":"2025-05-04T14:24:47.973002Z","shell.execute_reply.started":"2025-05-04T14:24:47.865851Z","shell.execute_reply":"2025-05-04T14:24:47.971607Z"}},"outputs":[{"name":"stdout","text":"Columns containing zeroes: ['Unnamed: 0', 'is_stop', 'order', 'tfidf_art', 'label3', 'label4', 'label5', 'is_ne', 'negative_conc', 'positive_conc', 'weak_subj', 'strong_subj', 'hyperbolic_terms', 'attitude_markers', 'kill_verbs', 'bias_lexicon', 'assertive_verbs', 'factive_verbs', 'report_verbs', 'implicative_verbs', 'hedges', 'boosters', 'affect ', 'posemo ', 'negemo ', 'anx ', 'anger ', 'sad ', 'social ', 'family ', 'friend ', 'female ', 'male ', 'cogproc ', 'insight ', 'cause ', 'discrep ', 'tentat ', 'certain ', 'differ ', 'percept ', 'see ', 'hear ', 'feel ', 'bio ', 'body ', 'health ', 'sexual ', 'ingest ', 'drives ', 'affiliation ', 'achieve ', 'power ', 'reward ', 'risk ', 'focuspast ', 'focuspresent ', 'focusfuture ', 'relativ ', 'motion ', 'space ', 'time ', 'work ', 'leisure ', 'home ', 'money ', 'relig ', 'death ', 'informal ', 'swear ', 'netspeak ', 'assent ', 'nonflu ', 'filler ', 'pos_ADJ', 'pos_ADP', 'pos_ADV', 'pos_AUX', 'pos_DET', 'pos_INTJ', 'pos_NOUN', 'pos_PRON', 'pos_PROPN', 'pos_SCONJ', 'pos_VERB', 'pos_X', 'dep_ROOT', 'dep_acl', 'dep_acomp', 'dep_advcl', 'dep_advmod', 'dep_agent', 'dep_amod', 'dep_appos', 'dep_attr', 'dep_aux', 'dep_auxpass', 'dep_cc', 'dep_ccomp', 'dep_compound', 'dep_conj', 'dep_csubj', 'dep_dative', 'dep_dep', 'dep_det', 'dep_dobj', 'dep_expl', 'dep_intj', 'dep_mark', 'dep_neg', 'dep_nmod', 'dep_npadvmod', 'dep_nsubj', 'dep_nsubjpass', 'dep_nummod', 'dep_oprd', 'dep_parataxis', 'dep_pcomp', 'dep_pobj', 'dep_poss', 'dep_preconj', 'dep_predet', 'dep_prep', 'dep_prt', 'dep_punct', 'dep_quantmod', 'dep_relcl', 'dep_xcomp', 'ne_CARDINAL', 'ne_DATE', 'ne_EVENT', 'ne_FAC', 'ne_GPE', 'ne_LANGUAGE', 'ne_LAW', 'ne_LOC', 'ne_MONEY', 'ne_NORP', 'ne_ORDINAL', 'ne_ORG', 'ne_PERCENT', 'ne_PERSON', 'ne_PRODUCT', 'ne_QUANTITY', 'ne_TIME', 'ne_WORK_OF_ART', 'negative_conc_context', 'positive_conc_context', 'weak_subj_context', 'strong_subj_context', 'hyperbolic_terms_context', 'attitude_markers_context', 'kill_verbs_context', 'bias_lexicon_context', 'assertive_verbs_context', 'factive_verbs_context', 'report_verbs_context', 'implicative_verbs_context', 'hedges_context', 'boosters_context', 'affect _context', 'posemo _context', 'negemo _context', 'anx _context', 'anger _context', 'sad _context', 'social _context', 'family _context', 'friend _context', 'female _context', 'male _context', 'cogproc _context', 'insight _context', 'cause _context', 'discrep _context', 'tentat _context', 'certain _context', 'differ _context', 'percept _context', 'see _context', 'hear _context', 'feel _context', 'bio _context', 'body _context', 'health _context', 'sexual _context', 'ingest _context', 'drives _context', 'affiliation _context', 'achieve _context', 'power _context', 'reward _context', 'risk _context', 'focuspast _context', 'focuspresent _context', 'focusfuture _context', 'relativ _context', 'motion _context', 'space _context', 'time _context', 'work _context', 'leisure _context', 'home _context', 'money _context', 'relig _context', 'death _context', 'informal _context', 'swear _context', 'netspeak _context', 'assent _context', 'nonflu _context', 'filler _context', 'pos_ADJ_context', 'pos_ADP_context', 'pos_ADV_context', 'pos_AUX_context', 'pos_DET_context', 'pos_INTJ_context', 'pos_NOUN_context', 'pos_PRON_context', 'pos_PROPN_context', 'pos_SCONJ_context', 'pos_VERB_context', 'pos_X_context', 'dep_ROOT_context', 'dep_acl_context', 'dep_acomp_context', 'dep_advcl_context', 'dep_advmod_context', 'dep_agent_context', 'dep_amod_context', 'dep_appos_context', 'dep_attr_context', 'dep_aux_context', 'dep_auxpass_context', 'dep_cc_context', 'dep_ccomp_context', 'dep_compound_context', 'dep_conj_context', 'dep_csubj_context', 'dep_dative_context', 'dep_dep_context', 'dep_det_context', 'dep_dobj_context', 'dep_expl_context', 'dep_intj_context', 'dep_mark_context', 'dep_neg_context', 'dep_nmod_context', 'dep_npadvmod_context', 'dep_nsubj_context', 'dep_nsubjpass_context', 'dep_nummod_context', 'dep_oprd_context', 'dep_parataxis_context', 'dep_pcomp_context', 'dep_pobj_context', 'dep_poss_context', 'dep_preconj_context', 'dep_predet_context', 'dep_prep_context', 'dep_prt_context', 'dep_punct_context', 'dep_quantmod_context', 'dep_relcl_context', 'dep_xcomp_context', 'ne_CARDINAL_context', 'ne_DATE_context', 'ne_EVENT_context', 'ne_FAC_context', 'ne_GPE_context', 'ne_LAW_context', 'ne_LOC_context', 'ne_MONEY_context', 'ne_NORP_context', 'ne_ORDINAL_context', 'ne_ORG_context', 'ne_PERCENT_context', 'ne_PERSON_context', 'ne_PRODUCT_context', 'ne_QUANTITY_context', 'ne_TIME_context', 'ne_WORK_OF_ART_context', 'ne_LANGUAGE_context']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# List the columns you want to process to fill since empty\ncolumns_to_fill = ['tfidf_art', 'MRCP_concretness_ratings', 'MRCP_Imagability_ratings',]\n\n# Fill NaN values in each column with that column's mean\nfor col in columns_to_fill:\n    mean_val = df[col].mean()\n    df[col] = df[col].fillna(mean_val)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:24:47.974613Z","iopub.execute_input":"2025-05-04T14:24:47.974930Z","iopub.status.idle":"2025-05-04T14:24:47.991431Z","shell.execute_reply.started":"2025-05-04T14:24:47.974906Z","shell.execute_reply":"2025-05-04T14:24:47.990255Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# columns to process\ncolumns_to_clean = ['ne_label', 'type']\n\n# Replace [''], [], or NaN with ['none'] in each column\nfor col in columns_to_clean:\n    df[col] = df[col].apply(lambda x: ['none'] if (x == [''] or x == [] or pd.isna(x)) else x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:24:47.992715Z","iopub.execute_input":"2025-05-04T14:24:47.994453Z","iopub.status.idle":"2025-05-04T14:24:48.251903Z","shell.execute_reply.started":"2025-05-04T14:24:47.994401Z","shell.execute_reply":"2025-05-04T14:24:48.251043Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"print(df['biased_words2'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:24:48.252656Z","iopub.execute_input":"2025-05-04T14:24:48.252872Z","iopub.status.idle":"2025-05-04T14:24:48.264802Z","shell.execute_reply.started":"2025-05-04T14:24:48.252856Z","shell.execute_reply":"2025-05-04T14:24:48.264080Z"}},"outputs":[{"name":"stdout","text":"biased_words2\n[]                              33655\n['claiming']                      439\n['claimed']                       316\n['claims']                        222\n['illegal', 'aliens']             209\n                                ...  \n['Firebombs']                       5\n['bungling']                        5\n['destructive']                     4\n['there', \"isn't\", 'anyone']        3\n['poison']                          3\nName: count, Length: 1623, dtype: int64\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"#check to see that all NAN cells have been filled. \nevaluate_file(df, duplicate_column=\"text\")  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:24:48.265965Z","iopub.execute_input":"2025-05-04T14:24:48.266362Z","iopub.status.idle":"2025-05-04T14:24:48.512082Z","shell.execute_reply.started":"2025-05-04T14:24:48.266298Z","shell.execute_reply":"2025-05-04T14:24:48.511208Z"}},"outputs":[{"name":"stdout","text":"\n=== File Info ===\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 65822 entries, 0 to 65821\nColumns: 301 entries, Unnamed: 0 to ne_LANGUAGE_context\ndtypes: bool(2), float64(4), int64(281), object(14)\nmemory usage: 150.3+ MB\nNone\n\n=== Duplicates in 'text' ===\nTotal duplicate values: 52531\n\n=== Zero Values ===\nTotal zero values: 17437766\n\n=== Empty (NaN) Cells ===\nTotal empty cells: 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"       Unnamed: 0                                           sentence  \\\n32706       54621  No sooner do states start getting together to ...   \n4645         7737  American Outdoor Brands Corp AOBC.O said on Th...   \n2217         3644  A Supreme Court filing lays bare the deep chas...   \n55548       92804  This dark new event revealed that America’s wh...   \n13535       22530  Conservative Republican Senator Ted Cruz favor...   \n\n         outlet                  topic    type  \\\n32706  Alternet            gun control    left   \n4645    Reuters            gun control  center   \n2217   Alternet                  taxes    left   \n55548  Alternet      white-nationalism    left   \n13535   Reuters  universal health care  center   \n\n                                                 article biased_words2  \\\n32706  No sooner do states start getting together to ...    ['assert']   \n4645   American Outdoor Brands Corp AOBC.O said on Th...            []   \n2217   A Supreme Court filing lays bare the deep chas...   ['wannabe']   \n55548  This dark new event revealed that America’s wh...      ['dark']   \n13535  Conservative Republican Senator Ted Cruz favor...            []   \n\n            text   text_low    pos  ... ne_NORP_context ne_ORDINAL_context  \\\n32706      start      start   VERB  ...               0                  0   \n4645        said       said   VERB  ...               0                  0   \n2217        lays       lays   VERB  ...               0                  0   \n55548    America    america  PROPN  ...               0                  0   \n13535  standards  standards   NOUN  ...               0                  0   \n\n      ne_ORG_context ne_PERCENT_context  ne_PERSON_context  \\\n32706              0                  0                  0   \n4645               1                  0                  0   \n2217               1                  0                  0   \n55548              0                  0                  0   \n13535              0                  0                  0   \n\n       ne_PRODUCT_context  ne_QUANTITY_context  ne_TIME_context  \\\n32706                   0                    0                0   \n4645                    0                    0                0   \n2217                    0                    0                0   \n55548                   0                    0                0   \n13535                   0                    0                0   \n\n       ne_WORK_OF_ART_context  ne_LANGUAGE_context  \n32706                       0                    0  \n4645                        0                    0  \n2217                        0                    0  \n55548                       0                    0  \n13535                       0                    0  \n\n[5 rows x 301 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>sentence</th>\n      <th>outlet</th>\n      <th>topic</th>\n      <th>type</th>\n      <th>article</th>\n      <th>biased_words2</th>\n      <th>text</th>\n      <th>text_low</th>\n      <th>pos</th>\n      <th>...</th>\n      <th>ne_NORP_context</th>\n      <th>ne_ORDINAL_context</th>\n      <th>ne_ORG_context</th>\n      <th>ne_PERCENT_context</th>\n      <th>ne_PERSON_context</th>\n      <th>ne_PRODUCT_context</th>\n      <th>ne_QUANTITY_context</th>\n      <th>ne_TIME_context</th>\n      <th>ne_WORK_OF_ART_context</th>\n      <th>ne_LANGUAGE_context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>32706</th>\n      <td>54621</td>\n      <td>No sooner do states start getting together to ...</td>\n      <td>Alternet</td>\n      <td>gun control</td>\n      <td>left</td>\n      <td>No sooner do states start getting together to ...</td>\n      <td>['assert']</td>\n      <td>start</td>\n      <td>start</td>\n      <td>VERB</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4645</th>\n      <td>7737</td>\n      <td>American Outdoor Brands Corp AOBC.O said on Th...</td>\n      <td>Reuters</td>\n      <td>gun control</td>\n      <td>center</td>\n      <td>American Outdoor Brands Corp AOBC.O said on Th...</td>\n      <td>[]</td>\n      <td>said</td>\n      <td>said</td>\n      <td>VERB</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2217</th>\n      <td>3644</td>\n      <td>A Supreme Court filing lays bare the deep chas...</td>\n      <td>Alternet</td>\n      <td>taxes</td>\n      <td>left</td>\n      <td>A Supreme Court filing lays bare the deep chas...</td>\n      <td>['wannabe']</td>\n      <td>lays</td>\n      <td>lays</td>\n      <td>VERB</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>55548</th>\n      <td>92804</td>\n      <td>This dark new event revealed that America’s wh...</td>\n      <td>Alternet</td>\n      <td>white-nationalism</td>\n      <td>left</td>\n      <td>This dark new event revealed that America’s wh...</td>\n      <td>['dark']</td>\n      <td>America</td>\n      <td>america</td>\n      <td>PROPN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13535</th>\n      <td>22530</td>\n      <td>Conservative Republican Senator Ted Cruz favor...</td>\n      <td>Reuters</td>\n      <td>universal health care</td>\n      <td>center</td>\n      <td>Conservative Republican Senator Ted Cruz favor...</td>\n      <td>[]</td>\n      <td>standards</td>\n      <td>standards</td>\n      <td>NOUN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 301 columns</p>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"df.head(35)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:24:48.514842Z","iopub.execute_input":"2025-05-04T14:24:48.515117Z","iopub.status.idle":"2025-05-04T14:24:48.540349Z","shell.execute_reply.started":"2025-05-04T14:24:48.515087Z","shell.execute_reply":"2025-05-04T14:24:48.539269Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"    Unnamed: 0                                           sentence    outlet  \\\n0            0  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n1            3  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n2            4  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n3            5  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n4            6  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n5            7  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n6            9  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n7           11  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n8           12  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n9           13  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n10          14  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n11          17  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n12          18  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n13          20  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n14          21  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n15          22  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n16          23  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n17          26  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n18          27  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n19          28  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n20          29  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n21          30  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n22          33  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n23          34  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n24          35  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n25          36  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n26          37  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n27          39  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n28          40  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n29          41  ...immigrants as criminals and eugenics, all o...     MSNBC   \n30          43  ...immigrants as criminals and eugenics, all o...     MSNBC   \n31          44  ...immigrants as criminals and eugenics, all o...     MSNBC   \n32          50  ...immigrants as criminals and eugenics, all o...     MSNBC   \n33          51  ...immigrants as criminals and eugenics, all o...     MSNBC   \n34          52  ...immigrants as criminals and eugenics, all o...     MSNBC   \n\n                topic   type  \\\n0         environment  right   \n1         environment  right   \n2         environment  right   \n3         environment  right   \n4         environment  right   \n5         environment  right   \n6         environment  right   \n7         environment  right   \n8         environment  right   \n9         environment  right   \n10        environment  right   \n11        environment  right   \n12        environment  right   \n13        environment  right   \n14        environment  right   \n15        environment  right   \n16        environment  right   \n17        gun control   left   \n18        gun control   left   \n19        gun control   left   \n20        gun control   left   \n21        gun control   left   \n22        gun control   left   \n23        gun control   left   \n24        gun control   left   \n25        gun control   left   \n26        gun control   left   \n27        gun control   left   \n28        gun control   left   \n29  white-nationalism   left   \n30  white-nationalism   left   \n31  white-nationalism   left   \n32  white-nationalism   left   \n33  white-nationalism   left   \n34  white-nationalism   left   \n\n                                              article  \\\n0   \"Orange Is the New Black\" star Yael Stone is r...   \n1   \"Orange Is the New Black\" star Yael Stone is r...   \n2   \"Orange Is the New Black\" star Yael Stone is r...   \n3   \"Orange Is the New Black\" star Yael Stone is r...   \n4   \"Orange Is the New Black\" star Yael Stone is r...   \n5   \"Orange Is the New Black\" star Yael Stone is r...   \n6   \"Orange Is the New Black\" star Yael Stone is r...   \n7   \"Orange Is the New Black\" star Yael Stone is r...   \n8   \"Orange Is the New Black\" star Yael Stone is r...   \n9   \"Orange Is the New Black\" star Yael Stone is r...   \n10  \"Orange Is the New Black\" star Yael Stone is r...   \n11  \"Orange Is the New Black\" star Yael Stone is r...   \n12  \"Orange Is the New Black\" star Yael Stone is r...   \n13  \"Orange Is the New Black\" star Yael Stone is r...   \n14  \"Orange Is the New Black\" star Yael Stone is r...   \n15  \"Orange Is the New Black\" star Yael Stone is r...   \n16  \"Orange Is the New Black\" star Yael Stone is r...   \n17  \"We have one beautiful law,\" Trump recently sa...   \n18  \"We have one beautiful law,\" Trump recently sa...   \n19  \"We have one beautiful law,\" Trump recently sa...   \n20  \"We have one beautiful law,\" Trump recently sa...   \n21  \"We have one beautiful law,\" Trump recently sa...   \n22  \"We have one beautiful law,\" Trump recently sa...   \n23  \"We have one beautiful law,\" Trump recently sa...   \n24  \"We have one beautiful law,\" Trump recently sa...   \n25  \"We have one beautiful law,\" Trump recently sa...   \n26  \"We have one beautiful law,\" Trump recently sa...   \n27  \"We have one beautiful law,\" Trump recently sa...   \n28  \"We have one beautiful law,\" Trump recently sa...   \n29  ...immigrants as criminals and eugenics, all o...   \n30  ...immigrants as criminals and eugenics, all o...   \n31  ...immigrants as criminals and eugenics, all o...   \n32  ...immigrants as criminals and eugenics, all o...   \n33  ...immigrants as criminals and eugenics, all o...   \n34  ...immigrants as criminals and eugenics, all o...   \n\n                         biased_words2                text  \\\n0                                   []              Orange   \n1                                   []                 New   \n2                                   []               Black   \n3                                   []                star   \n4                                   []                Yael   \n5                                   []               Stone   \n6                                   []          renouncing   \n7                                   []                U.S.   \n8                                   []               green   \n9                                   []                card   \n10                                  []              return   \n11                                  []              native   \n12                                  []           Australia   \n13                                  []               order   \n14                                  []               fight   \n15                                  []             climate   \n16                                  []              change   \n17   ['bizarre', 'characteristically']           beautiful   \n18   ['bizarre', 'characteristically']                 law   \n19   ['bizarre', 'characteristically']               Trump   \n20   ['bizarre', 'characteristically']            recently   \n21   ['bizarre', 'characteristically']                said   \n22   ['bizarre', 'characteristically']  characteristically   \n23   ['bizarre', 'characteristically']             bizarre   \n24   ['bizarre', 'characteristically']              syntax   \n25   ['bizarre', 'characteristically']             diction   \n26   ['bizarre', 'characteristically']           repeating   \n27   ['bizarre', 'characteristically']                word   \n28   ['bizarre', 'characteristically']           beautiful   \n29  ['criminals', 'fringe', 'extreme']          immigrants   \n30  ['criminals', 'fringe', 'extreme']           criminals   \n31  ['criminals', 'fringe', 'extreme']            eugenics   \n32  ['criminals', 'fringe', 'extreme']          considered   \n33  ['criminals', 'fringe', 'extreme']              fringe   \n34  ['criminals', 'fringe', 'extreme']             extreme   \n\n              text_low    pos  ... ne_NORP_context ne_ORDINAL_context  \\\n0               orange  PROPN  ...               0                  0   \n1                  new  PROPN  ...               0                  0   \n2                black  PROPN  ...               0                  0   \n3                 star   NOUN  ...               0                  0   \n4                 yael  PROPN  ...               0                  0   \n5                stone  PROPN  ...               0                  0   \n6           renouncing   VERB  ...               0                  0   \n7                 u.s.  PROPN  ...               0                  0   \n8                green    ADJ  ...               0                  0   \n9                 card   NOUN  ...               0                  0   \n10              return   VERB  ...               0                  0   \n11              native    ADJ  ...               0                  0   \n12           australia  PROPN  ...               0                  0   \n13               order   NOUN  ...               0                  0   \n14               fight   VERB  ...               0                  0   \n15             climate   NOUN  ...               0                  0   \n16              change   NOUN  ...               0                  0   \n17           beautiful    ADJ  ...               0                  0   \n18                 law   NOUN  ...               0                  0   \n19               trump  PROPN  ...               0                  0   \n20            recently    ADV  ...               0                  0   \n21                said   VERB  ...               0                  0   \n22  characteristically    ADV  ...               0                  0   \n23             bizarre    ADJ  ...               0                  0   \n24              syntax   NOUN  ...               0                  0   \n25             diction   NOUN  ...               0                  0   \n26           repeating   VERB  ...               0                  0   \n27                word   NOUN  ...               0                  0   \n28           beautiful    ADJ  ...               0                  0   \n29          immigrants   NOUN  ...               0                  0   \n30           criminals   NOUN  ...               0                  0   \n31            eugenics   NOUN  ...               0                  0   \n32          considered   VERB  ...               0                  0   \n33              fringe   NOUN  ...               0                  0   \n34             extreme   NOUN  ...               0                  0   \n\n   ne_ORG_context ne_PERCENT_context  ne_PERSON_context  ne_PRODUCT_context  \\\n0               0                  0                  0                   0   \n1               0                  0                  0                   0   \n2               0                  0                  1                   0   \n3               0                  0                  1                   0   \n4               0                  0                  1                   0   \n5               0                  0                  1                   0   \n6               0                  0                  1                   0   \n7               0                  0                  0                   0   \n8               0                  0                  0                   0   \n9               0                  0                  1                   0   \n10              0                  0                  1                   0   \n11              0                  0                  1                   0   \n12              0                  0                  0                   0   \n13              0                  0                  0                   0   \n14              0                  0                  0                   0   \n15              0                  0                  0                   0   \n16              0                  0                  0                   0   \n17              0                  0                  1                   0   \n18              0                  0                  1                   0   \n19              0                  0                  0                   0   \n20              0                  0                  1                   0   \n21              0                  0                  1                   0   \n22              0                  0                  0                   0   \n23              0                  0                  0                   0   \n24              0                  0                  0                   0   \n25              0                  0                  0                   0   \n26              0                  0                  0                   0   \n27              0                  0                  0                   0   \n28              0                  0                  0                   0   \n29              0                  0                  0                   0   \n30              0                  0                  0                   0   \n31              0                  0                  0                   0   \n32              0                  0                  0                   0   \n33              0                  0                  0                   0   \n34              0                  0                  0                   0   \n\n    ne_QUANTITY_context  ne_TIME_context  ne_WORK_OF_ART_context  \\\n0                     0                0                       1   \n1                     0                0                       1   \n2                     0                0                       1   \n3                     0                0                       1   \n4                     0                0                       1   \n5                     0                0                       0   \n6                     0                0                       0   \n7                     0                0                       0   \n8                     0                0                       0   \n9                     0                0                       0   \n10                    0                0                       0   \n11                    0                0                       0   \n12                    0                0                       0   \n13                    0                0                       0   \n14                    0                0                       0   \n15                    0                0                       0   \n16                    0                0                       0   \n17                    0                0                       0   \n18                    0                0                       0   \n19                    0                0                       0   \n20                    0                0                       0   \n21                    0                0                       0   \n22                    0                0                       0   \n23                    0                0                       0   \n24                    0                0                       0   \n25                    0                0                       0   \n26                    0                0                       0   \n27                    0                0                       0   \n28                    0                0                       0   \n29                    0                0                       0   \n30                    0                0                       0   \n31                    0                0                       0   \n32                    0                0                       0   \n33                    0                0                       0   \n34                    0                0                       0   \n\n    ne_LANGUAGE_context  \n0                     0  \n1                     0  \n2                     0  \n3                     0  \n4                     0  \n5                     0  \n6                     0  \n7                     0  \n8                     0  \n9                     0  \n10                    0  \n11                    0  \n12                    0  \n13                    0  \n14                    0  \n15                    0  \n16                    0  \n17                    0  \n18                    0  \n19                    0  \n20                    0  \n21                    0  \n22                    0  \n23                    0  \n24                    0  \n25                    0  \n26                    0  \n27                    0  \n28                    0  \n29                    0  \n30                    0  \n31                    0  \n32                    0  \n33                    0  \n34                    0  \n\n[35 rows x 301 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>sentence</th>\n      <th>outlet</th>\n      <th>topic</th>\n      <th>type</th>\n      <th>article</th>\n      <th>biased_words2</th>\n      <th>text</th>\n      <th>text_low</th>\n      <th>pos</th>\n      <th>...</th>\n      <th>ne_NORP_context</th>\n      <th>ne_ORDINAL_context</th>\n      <th>ne_ORG_context</th>\n      <th>ne_PERCENT_context</th>\n      <th>ne_PERSON_context</th>\n      <th>ne_PRODUCT_context</th>\n      <th>ne_QUANTITY_context</th>\n      <th>ne_TIME_context</th>\n      <th>ne_WORK_OF_ART_context</th>\n      <th>ne_LANGUAGE_context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>Fox News</td>\n      <td>environment</td>\n      <td>right</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>[]</td>\n      <td>Orange</td>\n      <td>orange</td>\n      <td>PROPN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>Fox News</td>\n      <td>environment</td>\n      <td>right</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>[]</td>\n      <td>New</td>\n      <td>new</td>\n      <td>PROPN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>Fox News</td>\n      <td>environment</td>\n      <td>right</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>[]</td>\n      <td>Black</td>\n      <td>black</td>\n      <td>PROPN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>Fox News</td>\n      <td>environment</td>\n      <td>right</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>[]</td>\n      <td>star</td>\n      <td>star</td>\n      <td>NOUN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>Fox News</td>\n      <td>environment</td>\n      <td>right</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>[]</td>\n      <td>Yael</td>\n      <td>yael</td>\n      <td>PROPN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>7</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>Fox News</td>\n      <td>environment</td>\n      <td>right</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>[]</td>\n      <td>Stone</td>\n      <td>stone</td>\n      <td>PROPN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>9</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>Fox News</td>\n      <td>environment</td>\n      <td>right</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>[]</td>\n      <td>renouncing</td>\n      <td>renouncing</td>\n      <td>VERB</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>11</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>Fox News</td>\n      <td>environment</td>\n      <td>right</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>[]</td>\n      <td>U.S.</td>\n      <td>u.s.</td>\n      <td>PROPN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>12</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>Fox News</td>\n      <td>environment</td>\n      <td>right</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>[]</td>\n      <td>green</td>\n      <td>green</td>\n      <td>ADJ</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>13</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>Fox News</td>\n      <td>environment</td>\n      <td>right</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>[]</td>\n      <td>card</td>\n      <td>card</td>\n      <td>NOUN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>14</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>Fox News</td>\n      <td>environment</td>\n      <td>right</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>[]</td>\n      <td>return</td>\n      <td>return</td>\n      <td>VERB</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>17</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>Fox News</td>\n      <td>environment</td>\n      <td>right</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>[]</td>\n      <td>native</td>\n      <td>native</td>\n      <td>ADJ</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>18</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>Fox News</td>\n      <td>environment</td>\n      <td>right</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>[]</td>\n      <td>Australia</td>\n      <td>australia</td>\n      <td>PROPN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>20</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>Fox News</td>\n      <td>environment</td>\n      <td>right</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>[]</td>\n      <td>order</td>\n      <td>order</td>\n      <td>NOUN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>21</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>Fox News</td>\n      <td>environment</td>\n      <td>right</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>[]</td>\n      <td>fight</td>\n      <td>fight</td>\n      <td>VERB</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>22</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>Fox News</td>\n      <td>environment</td>\n      <td>right</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>[]</td>\n      <td>climate</td>\n      <td>climate</td>\n      <td>NOUN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>23</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>Fox News</td>\n      <td>environment</td>\n      <td>right</td>\n      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n      <td>[]</td>\n      <td>change</td>\n      <td>change</td>\n      <td>NOUN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>26</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>Alternet</td>\n      <td>gun control</td>\n      <td>left</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>['bizarre', 'characteristically']</td>\n      <td>beautiful</td>\n      <td>beautiful</td>\n      <td>ADJ</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>27</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>Alternet</td>\n      <td>gun control</td>\n      <td>left</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>['bizarre', 'characteristically']</td>\n      <td>law</td>\n      <td>law</td>\n      <td>NOUN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>28</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>Alternet</td>\n      <td>gun control</td>\n      <td>left</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>['bizarre', 'characteristically']</td>\n      <td>Trump</td>\n      <td>trump</td>\n      <td>PROPN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>29</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>Alternet</td>\n      <td>gun control</td>\n      <td>left</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>['bizarre', 'characteristically']</td>\n      <td>recently</td>\n      <td>recently</td>\n      <td>ADV</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>30</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>Alternet</td>\n      <td>gun control</td>\n      <td>left</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>['bizarre', 'characteristically']</td>\n      <td>said</td>\n      <td>said</td>\n      <td>VERB</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>33</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>Alternet</td>\n      <td>gun control</td>\n      <td>left</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>['bizarre', 'characteristically']</td>\n      <td>characteristically</td>\n      <td>characteristically</td>\n      <td>ADV</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>34</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>Alternet</td>\n      <td>gun control</td>\n      <td>left</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>['bizarre', 'characteristically']</td>\n      <td>bizarre</td>\n      <td>bizarre</td>\n      <td>ADJ</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>35</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>Alternet</td>\n      <td>gun control</td>\n      <td>left</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>['bizarre', 'characteristically']</td>\n      <td>syntax</td>\n      <td>syntax</td>\n      <td>NOUN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>36</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>Alternet</td>\n      <td>gun control</td>\n      <td>left</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>['bizarre', 'characteristically']</td>\n      <td>diction</td>\n      <td>diction</td>\n      <td>NOUN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>37</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>Alternet</td>\n      <td>gun control</td>\n      <td>left</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>['bizarre', 'characteristically']</td>\n      <td>repeating</td>\n      <td>repeating</td>\n      <td>VERB</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>39</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>Alternet</td>\n      <td>gun control</td>\n      <td>left</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>['bizarre', 'characteristically']</td>\n      <td>word</td>\n      <td>word</td>\n      <td>NOUN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>40</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>Alternet</td>\n      <td>gun control</td>\n      <td>left</td>\n      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n      <td>['bizarre', 'characteristically']</td>\n      <td>beautiful</td>\n      <td>beautiful</td>\n      <td>ADJ</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>41</td>\n      <td>...immigrants as criminals and eugenics, all o...</td>\n      <td>MSNBC</td>\n      <td>white-nationalism</td>\n      <td>left</td>\n      <td>...immigrants as criminals and eugenics, all o...</td>\n      <td>['criminals', 'fringe', 'extreme']</td>\n      <td>immigrants</td>\n      <td>immigrants</td>\n      <td>NOUN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>43</td>\n      <td>...immigrants as criminals and eugenics, all o...</td>\n      <td>MSNBC</td>\n      <td>white-nationalism</td>\n      <td>left</td>\n      <td>...immigrants as criminals and eugenics, all o...</td>\n      <td>['criminals', 'fringe', 'extreme']</td>\n      <td>criminals</td>\n      <td>criminals</td>\n      <td>NOUN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>44</td>\n      <td>...immigrants as criminals and eugenics, all o...</td>\n      <td>MSNBC</td>\n      <td>white-nationalism</td>\n      <td>left</td>\n      <td>...immigrants as criminals and eugenics, all o...</td>\n      <td>['criminals', 'fringe', 'extreme']</td>\n      <td>eugenics</td>\n      <td>eugenics</td>\n      <td>NOUN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>50</td>\n      <td>...immigrants as criminals and eugenics, all o...</td>\n      <td>MSNBC</td>\n      <td>white-nationalism</td>\n      <td>left</td>\n      <td>...immigrants as criminals and eugenics, all o...</td>\n      <td>['criminals', 'fringe', 'extreme']</td>\n      <td>considered</td>\n      <td>considered</td>\n      <td>VERB</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>51</td>\n      <td>...immigrants as criminals and eugenics, all o...</td>\n      <td>MSNBC</td>\n      <td>white-nationalism</td>\n      <td>left</td>\n      <td>...immigrants as criminals and eugenics, all o...</td>\n      <td>['criminals', 'fringe', 'extreme']</td>\n      <td>fringe</td>\n      <td>fringe</td>\n      <td>NOUN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>52</td>\n      <td>...immigrants as criminals and eugenics, all o...</td>\n      <td>MSNBC</td>\n      <td>white-nationalism</td>\n      <td>left</td>\n      <td>...immigrants as criminals and eugenics, all o...</td>\n      <td>['criminals', 'fringe', 'extreme']</td>\n      <td>extreme</td>\n      <td>extreme</td>\n      <td>NOUN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>35 rows × 301 columns</p>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"#find all columns with words\ndef contains_words(series):\n    return series.astype(str).apply(lambda x: any(char.isalpha() for char in x)).any()\n\nword_columns = [col for col in df.columns if contains_words(df[col])]\n\nprint(\"Columns that contain words:\", word_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:24:48.541219Z","iopub.execute_input":"2025-05-04T14:24:48.541495Z","iopub.status.idle":"2025-05-04T14:25:04.188174Z","shell.execute_reply.started":"2025-05-04T14:24:48.541467Z","shell.execute_reply":"2025-05-04T14:25:04.187185Z"}},"outputs":[{"name":"stdout","text":"Columns that contain words: ['sentence', 'outlet', 'topic', 'type', 'article', 'biased_words2', 'text', 'text_low', 'pos', 'lemma', 'lemma_low', 'tag', 'dep', 'is_stop', 'is_ne', 'ne_label']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"#check datatypes\nprint(df.dtypes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:25:04.189409Z","iopub.execute_input":"2025-05-04T14:25:04.189682Z","iopub.status.idle":"2025-05-04T14:25:04.196770Z","shell.execute_reply.started":"2025-05-04T14:25:04.189664Z","shell.execute_reply":"2025-05-04T14:25:04.195507Z"}},"outputs":[{"name":"stdout","text":"Unnamed: 0                 int64\nsentence                  object\noutlet                    object\ntopic                     object\ntype                      object\n                           ...  \nne_PRODUCT_context         int64\nne_QUANTITY_context        int64\nne_TIME_context            int64\nne_WORK_OF_ART_context     int64\nne_LANGUAGE_context        int64\nLength: 301, dtype: object\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"#above is too broad so be more specific\ndef contains_list_or_string(col):\n    return col.apply(lambda x: isinstance(x, (list, str))).any()\n\nlist_or_string_columns = [col for col in df.columns if contains_list_or_string(df[col])]\n\nprint(\"Columns where any value is a list or string:\", list_or_string_columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:25:04.197845Z","iopub.execute_input":"2025-05-04T14:25:04.198095Z","iopub.status.idle":"2025-05-04T14:25:10.194335Z","shell.execute_reply.started":"2025-05-04T14:25:04.198076Z","shell.execute_reply":"2025-05-04T14:25:10.193444Z"}},"outputs":[{"name":"stdout","text":"Columns where any value is a list or string: ['sentence', 'outlet', 'topic', 'type', 'article', 'biased_words2', 'text', 'text_low', 'pos', 'lemma', 'lemma_low', 'tag', 'dep', 'ne_label']\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# List of columns to clean and encode\ncolumns_to_encode = list_or_string_columns\n\nlabel_encoders = {}\n\n# Step 1: Normalize the data: convert lists to strings\nfor col in columns_to_encode:\n    df[col] = df[col].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n\n# Apply LabelEncoder\nfor col in columns_to_encode:\n    le = LabelEncoder()\n    df[col + '_encoded'] = le.fit_transform(df[col].astype(str).fillna('unknown'))\n    label_encoders[col] = le\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:25:10.195358Z","iopub.execute_input":"2025-05-04T14:25:10.195629Z","iopub.status.idle":"2025-05-04T14:25:10.811535Z","shell.execute_reply.started":"2025-05-04T14:25:10.195608Z","shell.execute_reply":"2025-05-04T14:25:10.810654Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Vectorize multiple text columns\ntfidf_vectorizers = {}\ntfidf_matrices = []\n\nfor col in word_columns:\n    tfidf = TfidfVectorizer(stop_words='english', max_features=100)\n    matrix = tfidf.fit_transform(df[col].fillna('').astype(str))  # handle NaNs + force to string\n    tfidf_vectorizers[col] = tfidf\n    tfidf_matrices.append(matrix)\n\n# Combine all text vectors into one feature matrix\nfrom scipy.sparse import hstack\nX_text_combined = hstack(tfidf_matrices)\n\n#define target \ny = df['type'] \n#drop target so not leak into features\ndf = df.drop('type', axis=1)\n\n# Pull out all encoded numeric columns\nencoded_features_array = df[[col + '_encoded' for col in columns_to_encode]].values\n\n# Combine sparse TF-IDF matrix with dense numeric matrix\nX_combined = hstack([X_text_combined, encoded_features_array])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:25:10.812796Z","iopub.execute_input":"2025-05-04T14:25:10.813071Z","iopub.status.idle":"2025-05-04T14:25:17.274431Z","shell.execute_reply.started":"2025-05-04T14:25:10.813038Z","shell.execute_reply":"2025-05-04T14:25:17.273508Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"#train/test\nX_train, X_temp, y_train, y_temp = train_test_split(X_combined, y, test_size=0.4, random_state=54321)\nX_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=54321)\n\n# Assign \nfeatures_train = X_train\ntarget_train = y_train\n\nfeatures_valid = X_valid\ntarget_valid = y_valid\n\nfeatures_test = X_test\ntarget_test = y_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:25:17.275570Z","iopub.execute_input":"2025-05-04T14:25:17.275845Z","iopub.status.idle":"2025-05-04T14:25:17.339238Z","shell.execute_reply.started":"2025-05-04T14:25:17.275826Z","shell.execute_reply":"2025-05-04T14:25:17.338229Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Define models and their respective parameter grids\nmodels_and_params = {\n    'RandomForest': {\n        'model': RandomForestClassifier(),\n        'param_grid': {\n            'n_estimators': [50, 100, 200],\n            'max_depth': [None, 10, 20],\n            'random_state': [42]\n        }\n    },\n    'DecisionTree': {\n        'model': DecisionTreeClassifier(),\n        'param_grid': {\n            'max_depth': [3, 5, 10],\n            'random_state': [42]\n        }\n    },\n    'GradientBoosting': {\n        'model': GradientBoostingClassifier(),\n        'param_grid': {\n            'n_estimators': [50, 100, 200],\n            'learning_rate': [0.01, 0.1, 0.2],\n            'max_depth': [3, 5, 10],\n            'random_state': [42]\n        }\n    },\n    'LogisticRegression': {\n        'model': LogisticRegression(),\n        'param_grid': {}\n    },\n    'XGBClassifier': {\n        'model': xgb.XGBClassifier(),\n        'param_grid': {} \n    },\n    'LGBMClassifier': {\n        'model': lgb.LGBMClassifier(),\n        'param_grid': {}\n     },\n    'CatBoost': {\n        'model': CatBoostClassifier(),\n        'param_grid': {}\n    },\n}\n\ndef train_models(models_and_params, features_train, target_train):\n    trained_models = {}\n\n    for model_name, config in models_and_params.items():\n        print(f\"\\nRunning GridSearchCV for {model_name}...\")\n\n        # Hyperparameter tuning\n        grid_search = GridSearchCV(\n            estimator=config['model'],\n            param_grid=config['param_grid'],\n            cv=5,\n            scoring='f1_macro',\n            n_jobs=-1\n        )\n        grid_search.fit(features_train, target_train)\n\n        # Store the best model\n        best_model = grid_search.best_estimator_\n        trained_models[model_name] = best_model\n        print(f\"Best {model_name} Parameters: {grid_search.best_params_}\")\n\n    return trained_models\n\ndef evaluate_models(trained_models, features_train, target_train, features_test, target_test):\n    for model_name, model in trained_models.items():\n        print(f\"\\nEvaluating {model_name}...\")\n        evaluate_classification_model(model_name, model, features_train, target_train, features_test, target_test)\n\ndef evaluate_classification_model(model_name, model, features_train, target_train, features_test, target_test):\n    eval_stats = {}\n    fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n\n    for dataset_type, features, target in [('train', features_train, target_train), ('test', features_test, target_test)]:\n        eval_stats[dataset_type] = {}\n\n        pred_target = model.predict(features)\n        pred_proba = model.predict_proba(features)[:, 1]  # Use the correct dataset\n\n        # Compute metrics\n        f1_thresholds = np.arange(0, 1.01, 0.05)\n        f1_scores = [f1_score(target, pred_proba >= threshold) for threshold in f1_thresholds]\n        fpr, tpr, _ = roc_curve(target, pred_proba)\n        precision, recall, _ = precision_recall_curve(target, pred_proba)\n\n        # Aggregate results\n        eval_stats[dataset_type]['Accuracy'] = accuracy_score(target, pred_target)\n        eval_stats[dataset_type]['F1'] = f1_score(target, pred_target)\n        eval_stats[dataset_type]['ROC AUC'] = roc_auc_score(target, pred_proba)\n        eval_stats[dataset_type]['APS'] = average_precision_score(target, pred_proba)\n\n        color = 'blue' if dataset_type == 'train' else 'green'\n\n        # F1 Score Plot\n        ax = axs[0]\n        ax.plot(f1_thresholds, f1_scores, color=color, label=f'{dataset_type}, max={max(f1_scores):.2f}')\n        ax.set_xlabel('Threshold')\n        ax.set_ylabel('F1 Score')\n        ax.legend()\n        ax.set_title(f'F1 Score ({model_name})')\n\n        # ROC Curve\n        ax = axs[1]\n        ax.plot(fpr, tpr, color=color, label=f'{dataset_type}, AUC={roc_auc_score(target, pred_proba):.2f}')\n        ax.plot([0, 1], [0, 1], linestyle='--', color='gray')\n        ax.set_xlabel('False Positive Rate')\n        ax.set_ylabel('True Positive Rate')\n        ax.legend()\n        ax.set_title(f'ROC Curve ({model_name})')\n                # Precision-Recall Curve\n        ax = axs[2]\n        ax.plot(recall, precision, color=color, label=f'{dataset_type}, AP={average_precision_score(target, pred_proba):.2f}')\n        ax.set_xlabel('Recall')\n        ax.set_ylabel('Precision')\n        ax.legend()\n        ax.set_title(f'Precision-Recall Curve ({model_name})')\n\n    df_eval_stats = pd.DataFrame(eval_stats).round(2)\n    df_eval_stats = df_eval_stats.reindex(index=['Accuracy', 'F1', 'APS', 'ROC AUC'])\n\n    print(df_eval_stats)\n    plt.show()\n\n# Run training and evaluation separately\ntrained_models = train_models(models_and_params, features_train, target_train)\nevaluate_models(trained_models, features_train, target_train, features_test, target_test)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_best_model_by_auc(trained_models, features_test, target_test):\n    \"\"\"Find the model with the highest ROC-AUC score on the test set.\"\"\"\n    best_model = None\n    best_model_name = None\n    best_auc = 0\n\n    for model_name, model in trained_models.items():\n        pred_proba = model.predict_proba(features_test)[:, 1]\n        auc = roc_auc_score(target_test, pred_proba)\n\n        print(f\"{model_name} Test ROC-AUC: {auc:.4f}\")\n\n        if auc > best_auc:\n            best_auc = auc\n            best_model = model\n            best_model_name = model_name\n\n    print(f\"\\nBest Model: {best_model_name} with ROC-AUC: {best_auc:.4f}\")\n    return best_model, best_model_name\n\n# Find the best model based on ROC-AUC\nbest_model, best_model_name = find_best_model_by_auc(trained_models, features_test, target_test)\n\n# Compute ROC-AUC on the validation set using the best model\nbest_val_pred_proba = best_model.predict_proba(features_valid)[:, 1]\nauc_roc_val = roc_auc_score(target_valid, best_val_pred_proba)\n\nprint(f\"AUC-ROC Score for Best Model ({best_model_name}) on Validation Set: {auc_roc_val:.4f}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-04T14:27:21.771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}